{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing des sensibilités\n",
    "\n",
    "Le jeu de données utilisé ici est [sensitivity_godiva](sensitivity_godiva.d).\n",
    "\n",
    "Comme dans le cas précédent les résultats sont stockés dans la liste des réponses. Le `Browser` simplifie l'accès à des données grâce à la possibilité de sélection sur les métadonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valjean.eponine.tripoli4.parse import Parser\n",
    "\n",
    "t4vv_sg = 'sensitivity_godiva.d.res.ceav5'\n",
    "# scan du jeu de données\n",
    "t4p = Parser(t4vv_sg)\n",
    "# parsing du dernier batch\n",
    "t4pres = t4p.parse_from_index()\n",
    "# clefs disponibles dans le dictionnaire de résultats\n",
    "list(t4pres.res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lresp = t4pres.res['list_responses']\n",
    "len(lresp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre peut être plus grand qu'attendu par la lecture du jeu de données car chaque résultat consistue une entrée dans le dictionnaire, soit chaque résultat dont la valeur d'une métadonnée varie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, resp in enumerate(lresp):\n",
    "    print('Response {0}: clefs = {1}'.format(i, sorted(resp.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit donc un `Browser` pour nous faciliter la tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4b = t4pres.to_browser()\n",
    "print(t4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in list(t4b.keys()):\n",
    "    print(\"{0} -> {1}\".format(k, list(t4b.available_values(k))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples de sélection\n",
    "\n",
    "Pour la « démo », mais cela reflète probablement une future démarche de développement de test, on va récupérer des `Browser` et non les réponses directement. Cela permet notamment de sélectionner la bonne réponse pas à pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection 1 : réponses correspondant à l'U238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_u238 = t4b.filter_by(sensitivity_nucleus='U238')\n",
    "[\"{0} -> {1}\".format(k, list(b_u238.available_values(k))) for k in list(b_u238.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection 2 : réponses correspondant à des sections efficaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cs = t4b.filter_by(sensitivity_type='CROSS SECTION')\n",
    "[\"{0} -> {1}\".format(k, list(b_cs.available_values(k))) for k in list(b_cs.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection 3 : réponses correspondant au code de section efficace 52 ($n \\rightarrow \\gamma$ absorption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s42 = t4b.filter_by(sensitivity_reaction='SECTION CODE 42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s52 = t4b.filter_by(sensitivity_reaction='SECTION CODE 52')\n",
    "[\"{0} -> {1}\".format(k, list(b_s52.available_values(k))) for k in list(b_s52.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas on peut récupérer directement la réponse et l'utiliser grâce à la méthode `select_by`. L'argument `squeeze` permet de récupérer directement le dictionnaire correspondant à la réponse et non plus la liste correspondant à toutes les réponses satisfaisant la sélection. Cet argument n'est utilisable que s'il n'y a qu'une seule réponse satisfaisant la sélection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_s52 = t4b.select_by(sensitivity_reaction='SECTION CODE 52', squeeze=True)\n",
    "list(r_s52.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(r_s52['results'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas trois résultats sont disponibles et peuvent être transformés en datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valjean.eponine.tripoli4 import data_convertor as dcv\n",
    "from valjean.eponine.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nombre de batches utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nombre de batches utilisés :', r_s52['results']['used_batches'])\n",
    "ubres_s52 = dcv.convert_data(r_s52['results'], 'used_batches', name='section 52', what='used batches')\n",
    "print(ubres_s52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remarque* : il n'y a pas d'erreur sur le nombre de batches utilisés, le choix a été fait de l'initialiser à `np.nan`, ce qui ne bloque pas les tests. Il en est de même pour tous les résultats non affectés d'une erreur.\n",
    "\n",
    "\n",
    "#### Spectre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres_s52 = r_s52['results']['sensitivity_spectrum']\n",
    "print('spectre dont les clefs sont', list(sres_s52.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sres_s52['array'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres_s52['array'].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres_s52['array'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sres_s52['bins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les spectres ou les maillages les bins des sensibilités sont stockés dans un `OrderedDict`, comme dans le cas d'un spectre habituel. Seules les coordonnées sont changées, précisées par les bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssres_s52 = dcv.convert_data(r_s52['results'], 'sensitivity_spectrum', name='section 52', what='sensitivity')\n",
    "print(dssres_s52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans l'exemple précédent, il est possible de réduire le spectre aux seuls bins utilisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dssres_s52.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultat intégré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('résultat intégré :', r_s52['results']['integrated'])\n",
    "ires_s52 = dcv.convert_data(r_s52['results'], 'integrated', name='section 52', what='sensitivity')\n",
    "print(ires_s52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible ici de réduire les dimensions, ce qui reviendra à ne plus avoir de bins, vu qu'il n'y en a qu'un en énergie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ires_s52.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection 4 : première réponse (0) : les $k_\\mathrm{eff}$\n",
    "\n",
    "Deux types de résultats de $k_\\mathrm{eff}$ sont disponibles :\n",
    "* les $k_\\mathrm{eff}$ qui apparaissent comme les réponses standard dans le listing de sortie de Tripoli-4, qui comportent normalement trois évaluations : `KSTEP`, `KCOLL` et `KTRACK` ainsi que leurs corrélations et le résultat de leur combinaison, appelés ici $k_\\mathrm{eff}$ \"génériques\"\n",
    "* les $k_\\mathrm{eff}$ apparaissant le plus souvent en toute fin de listing, donc le *discard* est calculé automatiquement, de manière à en donner la meilleure estimation, appelés ici $k_\\mathrm{eff}$ \"automatiques\"\n",
    "\n",
    "#### $k_\\mathrm{eff}$ \"génériques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keffs = t4b.select_by(response_index=0)\n",
    "print(keffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres sélections sont également possibles, sans nécessité de connaître l'index de la réponses :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keffs = t4b.select_by(response_function='KEFFS')\n",
    "print(keffs)\n",
    "print(len(keffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a 7 $k_\\mathrm{eff}$ disponibles comme attendu (les 3 valeurs, les 3 combinaisons deux à deux et les corrélations associées et la combinaison des trois. Il est également possible de construire des datasets à partir de chacun de ces $k_\\mathrm{eff}$. Il faut cependant isoler les résultats un par un :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstep = t4b.select_by(response_function='KEFFS', keff_estimator='KSTEP', squeeze=True)\n",
    "print(list(kstep['results'].keys()))\n",
    "ds_kstep = dcv.convert_data(kstep['results'], 'keff', name='kstep', what='keff')\n",
    "print(ds_kstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstep_kcoll = t4b.select_by(response_function='KEFFS', keff_estimator='KSTEP-KCOLL', squeeze=True)\n",
    "print(list(kstep_kcoll['results'].keys()))\n",
    "print(dcv.convert_data(kstep_kcoll['results'], 'keff', name='kstep-kcoll', what='keff'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas la corrélation est atteignable par :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dcv.convert_data(kstep_kcoll['results'], 'keff', correlation=True,\n",
    "      name='kstep-kcoll', what='correlation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : elle n'a pas d'erreur.\n",
    "\n",
    "\n",
    "#### $k_\\mathrm{eff}$ \"automatiques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keffs = t4b.select_by(response_type='keff_auto')\n",
    "print(keffs)\n",
    "print(len(keffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas il y a un autre estimateur en plus : MACRO KCOLL. À noter également la présence du nombre de batches *discarded* puisqu'il est calculé par Tripoli-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection 5 : boucle sur toutes les réponses\n",
    "\n",
    "Il est toujours possible de faire une boucle sur toutes les réponses, dans l'ordre dans lequel elles apparaissent dans le jeu de données (par exemple pour les besoins de la non-régression).\n",
    "\n",
    "#### Premier choix : boucle directe sur la liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resp in lresp:\n",
    "    print(\"Response function: {0}, response type: {1}, r_index = {2}, s_index = {3}\\n results keys: {4}\"\n",
    "          .format(resp['response_function'], resp['response_type'], resp['response_index'],\n",
    "                  resp.get('sensitivity_index', None), list(resp['results'].keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas les réponses sont déjà « aplaties »... mais le gros avantage est que les métadonnées sont accessibles pour chaque réponse (même les communes), il n'y a donc pas de perte d'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deuxième choix : boucle grâce au `Browser` (boucle sur le 'response_index'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rindex = list(t4b.available_values('response_index'))\n",
    "val_rindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind in val_rindex:\n",
    "    b_ind = t4b.filter_by(response_index=ind)\n",
    "    print(\"Nombre de 'scores' par réponse =\", b_ind)\n",
    "    for resp in b_ind.content:\n",
    "        print(\"Response function = {0}, sensitivity type = {1}, sensitivity index = {2}\"\n",
    "              .format(resp['response_function'], resp.get('sensitivity_type', None),\n",
    "                      resp.get('sensitivity_index', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas présent on obtient tous les résultats formatés de la même manière dans le listing de sortie de Tripoli-4, les $k_\\mathrm{eff}$ \"automatiques\" n'y sont donc pas. Ils peuvent cependant être également obtenus dans la boucle, à condition de boucler sur `'index'` au lieu de `'response_index'` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_index = list(t4b.available_values('index'))\n",
    "print('index :', val_index)\n",
    "for ind in val_index:\n",
    "    b_ind = t4b.filter_by(index=ind)\n",
    "    print(\"Nombre de 'scores' par réponse =\", b_ind)\n",
    "    for resp in b_ind.content:\n",
    "        print(\"Response function = {0}, response type = {1}, sensitivity type = {2}, sensitivity index = {3}\"\n",
    "              .format(resp.get('response_function', None), resp['response_type'],\n",
    "                      resp.get('sensitivity_type', None), resp.get('sensitivity_index', None)))\n",
    "        if resp.get('response_function') is None:\n",
    "            print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemples de comparaison de $k_\\mathrm{eff}$\n",
    "\n",
    "On va faire différents tests pour comparer les valeurs de $k_\\mathrm{eff}$ :\n",
    "\n",
    "* `TestEqual` qui vérifie que les datasets sont égaux (ce test est plutôt prévu pour des valeurs entières comme les nombres de batches)\n",
    "* `TestApproxEqual` qui vérifie que les datasets sont approximativement égaux (pertinent pour les `float` pour lesquels on n'a pas d'erreur associée, les corrélations de $k_\\mathrm{eff}$ par exemple\n",
    "* `TestStudent` dans le cas où l'on veut prendre en compte les erreurs sur les valeurs\n",
    "\n",
    "Pour tous ces tests il faut définir une référence, qui sera en fait le premier dataset donné.\n",
    "\n",
    "Pour les exemples ci-dessous on choisit les $k_\\mathrm{eff}$ des réponses (`response_function='KEFFS'`). On prendra comme référence le `KSTEP` pour plus de facilités car c'est le premier résultat donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = t4b.filter_by(response_function='KEFFS')\n",
    "print('estimators values:', list(sb.available_values('keff_estimator')))\n",
    "dsets = []\n",
    "for keff in sb.content:\n",
    "    dsets.append(dcv.convert_data(keff['results'], 'keff', name=keff['keff_estimator'], what='keff'))\n",
    "print(dsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les tests et la possibilité d'en faire des représentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valjean.gavroche.test import TestEqual, TestApproxEqual\n",
    "from valjean.gavroche.stat_tests.student import TestStudent\n",
    "from valjean.javert.representation import FullRepresenter\n",
    "from valjean.javert.rst import RstFormatter\n",
    "from valjean.javert.mpl import MplPlot\n",
    "from valjean.javert.verbosity import Verbosity\n",
    "\n",
    "frepr = FullRepresenter()\n",
    "rstformat = RstFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_res = TestEqual(*dsets, name='TestEqual', description='Test le TestEqual sur les keff').evaluate()\n",
    "print(bool(teq_res))  # expected: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqrepr = frepr(teq_res, verbosity=Verbosity.FULL_DETAILS)  # il s'agit d'une liste de templates\n",
    "print(eqrepr, len(eqrepr))\n",
    "eqrst = rstformat.template(eqrepr[0])\n",
    "print(eqrst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstud_res = TestStudent(*dsets, name='TestStudent', description='Test le TestStudent sur les keff').evaluate()\n",
    "print(bool(tstud_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studrepr = frepr(tstud_res, verbosity=Verbosity.FULL_DETAILS)  # il s'agit d'une liste de templates\n",
    "print(len(studrepr), [type(t) for t in studrepr])\n",
    "studrst = rstformat.template(studrepr[0])\n",
    "print(studrst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'avoir une représentation de ce test plus lisible en créant deux `Dataset` : l'un contenant tous ces résultats, l'autre, qui servira de référence, le KSTEP autant de fois que d'éléments dans le premier. Dans ce cas la représentation permettra également d'avoir une représentation graphique.\n",
    "\n",
    "Pour les bins, le plus simple est de donner les noms des estimateurs, actuellement stocké comme `'name'`.\n",
    "\n",
    "Cette forme permet également de comparer ces mêmes valeurs pour différentes versions de Tripoli-4 par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_test = Dataset(value=np.array([k.value for k in dsets]),\n",
    "                    error=np.array([k.error for k in dsets]),\n",
    "                    bins=OrderedDict([('estimator', np.array([k.name for k in dsets]))]),\n",
    "                    name='Test', what='keff')\n",
    "print(dset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait maintenant le `Dataset` de référence.\n",
    "\n",
    "**ATTENTTION : pour pouvoir comparer des `Dataset` il faut qu'ils aient les mêmes bins. La référence (qui ne contient que KSTEP) doit donc avoir tous les estimateurs dans les bins (les valeurs étant bien sûr celles du KSTEP).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_ref = Dataset(value=np.array([ds_kstep.value for _ in dsets]),\n",
    "                   error=np.array([ds_kstep.error for _ in dsets]),\n",
    "                   bins=OrderedDict([('estimator', np.array([k.name for k in dsets]))]),\n",
    "                   name='Ref (KSTEP)', what='keff')\n",
    "print(dset_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation avec le `TestStudent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstudRT_res = TestStudent(dset_ref, dset_test, name='TestStudent', description='Test le TestStudent sur les keff').evaluate()\n",
    "print(bool(tstudRT_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studRTtemp = frepr(tstudRT_res, verbosity=Verbosity.FULL_DETAILS)  # il s'agit d'une liste de templates\n",
    "print(len(studRTtemp), [type(t) for t in studRTtemp])\n",
    "studRTrst = rstformat.template(studRTtemp[1])\n",
    "print(studRTrst)\n",
    "mpl = MplPlot(studRTtemp[0]).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation avec le `TestApproxEqual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscorr = []\n",
    "for keff in sb.content:\n",
    "    dscorr.append(dcv.convert_data(keff['results'], 'keff', correlation=True,\n",
    "                                   name=keff['keff_estimator'], what='correlation'))\n",
    "dscorr_test = Dataset(value=np.array([k.value for k in dscorr]),\n",
    "                      error=np.array([k.error for k in dscorr]),\n",
    "                      bins=OrderedDict([('estimator', np.array([k.name for k in dscorr]))]),\n",
    "                      name='Test', what='correlation')\n",
    "dscorr_kstep = dcv.convert_data(kstep['results'], 'keff', correlation=True,\n",
    "                                name=keff['keff_estimator'], what='correlation')\n",
    "dscorr_ref = Dataset(value=np.array([dscorr_kstep.value for _ in dscorr]),\n",
    "                     error=np.array([dscorr_kstep.error for _ in dscorr]),\n",
    "                     bins=OrderedDict([('estimator', np.array([k.name for k in dscorr]))]),\n",
    "                     name='Ref (KSTEP)', what='correlation')\n",
    "print(dscorr)\n",
    "taeqRT_res = TestApproxEqual(\n",
    "    dscorr_ref, dscorr_test, name='TestApproxEqual',\n",
    "    description='Test le TestApproxEqual sur les correlations entre estimations de keff').evaluate()\n",
    "print(bool(taeqRT_res))\n",
    "aeqRTtemp = frepr(taeqRT_res, verbosity=Verbosity.FULL_DETAILS)  # il s'agit d'une liste de templates\n",
    "print(len(aeqRTtemp), [type(t) for t in aeqRTtemp])\n",
    "aeqRTrst = rstformat.template(aeqRTtemp[1])\n",
    "print(aeqRTrst)\n",
    "mpl = MplPlot(aeqRTtemp[0]).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation avec `TestEqual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsub = []\n",
    "for keff in sb.content:\n",
    "    dsub.append(dcv.convert_data(keff['results'], 'used_batches',\n",
    "                                 name=keff['keff_estimator'], what='used batches'))\n",
    "dsub_test = Dataset(value=np.array([k.value for k in dsub]),\n",
    "                    error=np.array([k.error for k in dsub]),\n",
    "                    bins=OrderedDict([('estimator', np.array([k.name for k in dsub]))]),\n",
    "                    name='Test', what='used batches')\n",
    "dsub_kstep = dcv.convert_data(kstep['results'], 'used_batches',\n",
    "                              name=keff['keff_estimator'], what='used batches')\n",
    "dsub_ref = Dataset(value=np.array([dsub_kstep.value for _ in dsub]),\n",
    "                   error=np.array([dsub_kstep.error for _ in dsub]),\n",
    "                   bins=OrderedDict([('estimator', np.array([k.name for k in dsub]))]),\n",
    "                   name='Ref (KSTEP)', what='used batches')\n",
    "print(dsub)\n",
    "teqRT_res = TestEqual(\n",
    "    dsub_ref, dsub_test, name='TestEqual',\n",
    "    description='Test le TestEqual sur les batches utilisés pour les estimations de keff').evaluate()\n",
    "print(bool(teqRT_res))\n",
    "eqRTtemp = frepr(teqRT_res, verbosity=Verbosity.FULL_DETAILS)  # il s'agit d'une liste de templates\n",
    "print(len(eqRTtemp), [type(t) for t in eqRTtemp])\n",
    "eqRTrst = rstformat.template(eqRTtemp[1])\n",
    "print(eqRTrst)\n",
    "mpl = MplPlot(eqRTtemp[0]).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut comparer les résultats à d'autres obtenus à partir d'une autre version de Tripoli-4 par exemple, on peut faire un dataset unique et le mettre dans un test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
